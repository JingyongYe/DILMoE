# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from typing import TYPE_CHECKING, Any, Optional, Tuple, Union, cast
from torch import Tensor
from torch.nn import ModuleList
import torch.nn.functional as F
import numpy as np

from ..impls import communicate as C
from ..impls.generalized_fast_dispatch import fast_encode, fast_decode, extract_critical
from ..impls.overlap import a2a_ffn_overlap_forward
from . import losses

from ..gates.top import LinearTopKGate

class MOELayer(torch.nn.Module):
    """Tutel optimized MOELayer
    """
    @staticmethod
    def global_expert_count(num_local_experts, group=None):
        if not isinstance(num_local_experts, int):
            num_local_experts = -int(1 / (num_local_experts + 1e-5))
        world_size = C.get_world_size(group)
        if num_local_experts == 0:
            raise Exception("Invalid value of num_local_experts: %d" % num_local_experts)
        if num_local_experts > 0:
            return num_local_experts * world_size
        assert world_size % -num_local_experts == 0, f"Excepting {-num_local_experts} devices to share an expert param, while global device count is {world_size}."
        return world_size // -num_local_experts
    
    @staticmethod
    def local_expert_count(num_global_experts, group=None):       
        world_size = C.get_world_size(group)
        assert num_global_experts % world_size == 0, f"Excepting {num_global_experts} devices to share an expert param, while global device count is {world_size}."
        return num_global_experts // world_size

    def _load_from_state_dict(self, state_dict, prefix, *args, **kwargs):
        buff_name = prefix + '_num_global_experts'
        if buff_name not in state_dict:
            logging.warning(f"\033[31mYou are loading a legacy format of checkpoint with at least one Tutel MoE layer inside, which wouldn't support new Tutel feature allowing the number of experts per checkpoint file to mutate.\033[0m")
            logging.warning(f"\033[31m  The next time you overwrite it with new checkpoint, the recording format will be updated automatically.\033[0m")
            logging.warning(f"\033[31m  However, the new format won't be compatible with early Tutel versions, unless you force loading it with `model.load_state_dict(.., strict=False)`.\033[0m")
            state_dict[buff_name] = self._num_global_experts
        else:
            state_experts, expect_experts = int(state_dict[buff_name]), self.num_global_experts
            # assert state_experts == expect_experts, "Failed to load state from checkpoint: the number of global experts mismatch (%s <- %s)" % (expect_experts, state_experts)

        buff_name = prefix + '_max_num_global_experts'
        if buff_name not in state_dict:
            logging.warning(f"\033[31mYou are loading a legacy format of checkpoint with at least one Tutel MoE layer inside, which wouldn't support new Tutel feature allowing the number of experts per checkpoint file to mutate.\033[0m")
            logging.warning(f"\033[31m  The next time you overwrite it with new checkpoint, the recording format will be updated automatically.\033[0m")
            logging.warning(f"\033[31m  However, the new format won't be compatible with early Tutel versions, unless you force loading it with `model.load_state_dict(.., strict=False)`.\033[0m")
            state_dict[buff_name] = self._max_num_global_experts
        else:
            state_experts, expect_experts = int(state_dict[buff_name]), self.max_num_global_experts
            assert state_experts == expect_experts, "Failed to load state from checkpoint: the max number of global experts mismatch (%s <- %s)" % (expect_experts, state_experts)

        for name, param in self.experts.named_parameters():
            buff_name = prefix + 'experts.' + name
            assert buff_name in state_dict, "Could not find parameter `%s` in state_dict." % buff_name
            if state_dict[buff_name].numel() == param.numel():
                state_dict[buff_name] = state_dict[buff_name].view(param.shape)
        return super()._load_from_state_dict(state_dict, prefix, *args, **kwargs)

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        return super().state_dict(destination, prefix, keep_vars)

    @property
    def num_global_experts(self):
        return int(self._num_global_experts)
    
    @property
    def max_num_global_experts(self):
        return int(self._max_num_global_experts)

    def __init__(
        self,
        gate_type,
        model_dim: int,
        experts=None,
        scan_expert_func=None,
        result_func=None,
        group=None,
        seeds=None,
        a2a_ffn_overlap_degree=1,
        is_postscore=True,
        batch_prioritized_routing=False,
        normalize_gate=True,
        is_gshard_loss=True,
        parallel_type='adaptive:1',
        use_2dh=False,
        one_score_gate=False,
        normalize_one_score_gate=False,
        value_norm_weighted=False,
        update_momentum=0.0,
        # DDG-ALS åˆ›æ–°å‚æ•°
        enable_ddg_als=False,  # å¯ç”¨åŠ¨æ€è§£è€¦é—¨æ§å’Œè‡ªé€‚åº”æŸå¤±è°ƒåº¦
        decoupled_gating=False,  # è§£è€¦é—¨æ§å¼€å…³
        adaptive_loss_schedule=False,  # è‡ªé€‚åº”æŸå¤±è°ƒåº¦å¼€å…³
        expert_momentum=0.9,  # ä¸“å®¶æ¿€æ´»åŠ¨é‡
        temperature_init=1.0,  # åˆå§‹æ¸©åº¦
        temperature_decay=0.995,  # æ¸©åº¦è¡°å‡ç‡
        exploration_bonus_init=0.1,  # åˆå§‹æ¢ç´¢å¥–åŠ±
        exploration_decay=0.99,  # æ¢ç´¢å¥–åŠ±è¡°å‡
        loss_schedule_warmup=1000,  # æŸå¤±è°ƒåº¦é¢„çƒ­æ­¥æ•°
        # share_value=False,
        **kwargs
    ):
        super().__init__()
        assert model_dim % 2 == 0, "Model_dim (%s) must be even value, while this Model_dim mod 2 > 0." % model_dim
        group = group or dist.group.WORLD

        if 'pad_samples' in kwargs:
            logging.warning(f"`pad_samples` option in Tutel Moe-layer has been deprecated, as Tutel always assumes `pad_samples=False` for better efficiency.")
            kwargs.pop('pad_samples')
        for k in kwargs:
            raise Exception('Unrecognized argument provided to Tutel Moe-layer: %s' % k)

        self.group = group
        self.result_func = result_func
        self.skip_moe = (int(os.environ.get('SKIP_MOE', '0')) != 0)

        # initially set max number equal to local number
        self.num_local_experts = experts.pop('count_per_node', 1)
        self.max_num_local_experts = self.num_local_experts
        self.register_buffer('_num_global_experts', torch.tensor(MOELayer.global_expert_count(self.num_local_experts, self.group)))
        self.register_buffer('_max_num_global_experts', torch.tensor(MOELayer.global_expert_count(self.max_num_local_experts, self.group)))

        self.world_size = C.get_world_size(self.group)
        # in fact this will not happen
        if self.num_global_experts < self.world_size:
            self.sharded_count = self.world_size // self.num_global_experts
            self.num_local_experts = 1
        else:
            self.sharded_count = 1

        self.auto_parallel, self.adaptive_degree, self.use_model_parallel = False, self.sharded_count, True
        self.valid_rs = [0] + [i for i in range(1, self.sharded_count + 1) if self.sharded_count % i == 0]
        # å®é™…ä¸Švalid_rsæ€»æ˜¯[0]

        if parallel_type.startswith('adaptive:'):
            self.adaptive_degree = int(parallel_type[parallel_type.index(':') + 1:])
            self.adaptive_degree = min(max(self.adaptive_degree, 0), self.sharded_count)
            if self.adaptive_degree not in self.valid_rs:
                raise Exception("Unexpected value of adaptive_degree: %d, expecting a candidate within %s." % (self.adaptive_degree, self.valid_rs))
        elif self.sharded_count == 1:
            pass
        elif parallel_type in ('data', 'model'):
            self.adaptive_degree = 1 if parallel_type == 'data' else self.sharded_count
        elif parallel_type == 'auto':
            self.adaptive_degree = 1
        else:
            raise Exception('Unrecognized parallel type specified: %s' % parallel_type)

        self.model_dim = model_dim

        self.is_postscore = is_postscore
        self.batch_prioritized_routing = batch_prioritized_routing
        if int(os.environ.get('BATCH_PRIO', 0)) != 0:
            self.batch_prioritized_routing = True
        self.normalize_gate = normalize_gate
        self.is_gshard_loss = is_gshard_loss

        self.a2a_ffn_overlap_degree = a2a_ffn_overlap_degree
        self.use_2dh = use_2dh

        if seeds is not None and seeds[1] is not None:
            torch.manual_seed(seeds[1])
            
        # self.share_value = share_value

        # GAMoE requires define the gates first.
        if isinstance(gate_type, str) and (not gate_type.startswith("GMGate")):
            
            assert re.match(r'^Top[0-9]+Gate$', gate_type), "Unrecognized gate_type: %s" % gate_type
            top_k = int(gate_type[3:-4])
            logging.warning(f"gate_type value `{gate_type}` in Tutel Moe-layer has been deprecated, please use gate_type = {{'type': 'top', 'k': {top_k}}} instead.")
            gate_type = {'type': 'top', 'k': top_k}

        elif isinstance(gate_type, str) and gate_type.startswith("GMGate"):
            max_K = int(gate_type[6:])
            gate_type = {'type': 'gated_multi_gate', 'max_expert_num': max_K}
            # if use GAMoE, set the max number of global experts to the parameters in config
            
        if 'max_expert_num' in gate_type:
            self._max_num_global_experts.data = torch.tensor(int(gate_type['max_expert_num']))
            self.max_num_local_experts = self.local_expert_count(self.max_num_global_experts, self.group)

        if not isinstance(gate_type, list):
            gate_type = [gate_type]

        self.gates = []
        for gi, single_gate_type in enumerate(gate_type):
            gate_type = single_gate_type['type']
            single_gate_type.pop('type')
            assert re.match(r'[a-zA-Z0-9\_]+', gate_type), "Gate type must only include digits, letters and underline characters."

            if seeds is not None and seeds[0] is not None:
                torch.manual_seed(seeds[0] + gi)
            try:
                single_gate = importlib.import_module(f'...gates.{gate_type}', __name__)
            except ModuleNotFoundError:
                raise Exception("Unrecognized gate_type: %s" % gate_type)

            gate_module = single_gate.Gate(model_dim=self.model_dim, num_global_experts=self.num_global_experts, normalize_one_score_gate=normalize_one_score_gate, **single_gate_type)
            if not hasattr(gate_module, 'gate_noise'):
                gate_module.gate_noise = single_gate_type.get('gate_noise', 0.0)
            if not hasattr(gate_module, 'capacity_factor'):
                gate_module.capacity_factor = single_gate_type.get('capacity_factor', float(os.environ.get('CAP_FACTOR', 1.0)))

            self.gates += [gate_module]
        print("Gate types: ", [x.__class__.__name__ for x in self.gates])
        self.gates = ModuleList(self.gates)

        experts_type = experts.pop('type')
        if experts_type == 'custom':
            self.experts = cast(ModuleList, experts['module'])
        else:
            assert re.match(r'[a-zA-Z0-9\_]+', experts_type), "Expert type must only include digits, letters and underline characters."
            try:
                fused_experts = importlib.import_module(f'...experts.{experts_type}', __name__)
            except ModuleNotFoundError:
                raise Exception('Builtin expert type is not recognized: %s' % experts_type)

            if experts_type == 'ffn':
                assert 'fused_custom_fn' not in experts, "`fused_custom_fn` option for Tutel Moe-layer has been deprecated, please follows helloworld_from_scratch.py for custom construction instead."
                assert 'implicit_dropout_p' not in experts, "`implicit_dropout_p` option for Tutel Moe-layer has been deprecated, please use torch.nn.Dropout(p=implicit_dropout_p) on custom activation_fn (for fc1_dropout) and after Tutel Moe-layer (for fc2_dropout) instead."

            self.experts = fused_experts.ExpertModule(**experts)

        self.experts.update(self)

        if scan_expert_func is not None:
            for n, p in self.experts.named_parameters():
                scan_expert_func(n, p)
        for n, p in self.experts.named_parameters():
            setattr(p, '_tutel_expert', True)

        
        #ä¸“é—¨ç”¨äºEMoE
        self.one_score_gate = one_score_gate
        if self.one_score_gate:
            self.update_momentum = update_momentum
            assert isinstance(self.gates[0], LinearTopKGate), "only simple gate is supported"
            print("Using one score gate with momentum {}, freeze gate weight!".format(self.update_momentum))
            self.normalize_one_score_gate = normalize_one_score_gate
            self.gates[0].wg.weight.require_grad = False
            self.value_norm_weighted = value_norm_weighted
            if self.value_norm_weighted:
                self.register_buffer('expert_importance', torch.ones(self.num_global_experts) / self.num_global_experts)
        
        # DDG-ALS åˆ›æ–°æœºåˆ¶åˆå§‹åŒ–
        self.enable_ddg_als = enable_ddg_als
        self.decoupled_gating = decoupled_gating
        self.adaptive_loss_schedule = adaptive_loss_schedule
        
        if self.enable_ddg_als:
            print(f"ğŸš€ Initializing DDG-ALS: Decoupled Gating={decoupled_gating}, Adaptive Loss={adaptive_loss_schedule}")
            
            # åŠ¨æ€è§£è€¦é—¨æ§ç›¸å…³å‚æ•°
            if self.decoupled_gating:
                # ä¸“å®¶é€‰æ‹©æƒé‡ (ç”¨äºè·¯ç”±)
                self.selection_gate = torch.nn.Linear(model_dim, self.max_num_global_experts, bias=False)
                # è¾“å‡ºèåˆæƒé‡ (ç”¨äºåŠ æƒè¾“å‡ºï¼ŒåŸºäºåŠ¨é‡æ›´æ–°)
                self.register_buffer('fusion_weights', torch.ones(self.max_num_global_experts) / self.max_num_global_experts)
                self.expert_momentum = expert_momentum
                print(f"  âœ“ Decoupled gating initialized with momentum {expert_momentum}")
            
            # è‡ªé€‚åº”æ¸©åº¦æœºåˆ¶
            self.register_buffer('current_temperature', torch.tensor(temperature_init))
            self.temperature_decay = temperature_decay
            
            # ä¸“å®¶æ¿€æ´»å†å²è·Ÿè¸ª
            self.register_buffer('expert_activation_history', torch.zeros(self.max_num_global_experts))
            self.register_buffer('expert_usage_count', torch.zeros(self.max_num_global_experts))
            
            # è‡ªé€‚åº”æŸå¤±è°ƒåº¦å‚æ•°
            if self.adaptive_loss_schedule:
                self.register_buffer('training_step', torch.tensor(0))
                self.exploration_bonus_init = exploration_bonus_init
                self.exploration_decay = exploration_decay
                self.loss_schedule_warmup = loss_schedule_warmup
                self.register_buffer('current_aux_weight', torch.tensor(0.001))  # åˆå§‹å¾ˆå°çš„è¾…åŠ©æŸå¤±
                print(f"  âœ“ Adaptive loss scheduling initialized with warmup {loss_schedule_warmup} steps")
        
        self.record_routing = False
        
        if seeds is not None and len(seeds) > 2 and seeds[2] is not None:
            torch.manual_seed(seeds[2])

    def extra_repr(self):
        return 'Top-K(s) = %s, Total-Experts = %d [managed by %d device(s)],' % (
            [f'k={x.top_k}, noise={x.gate_noise}' for x in self.gates],
            self.num_global_experts,
            self.world_size,
        )

    def get_parameter_iterator(self, param_type):
        if param_type == 'gate':
            return self.gates.named_parameters()
        elif param_type == 'local_experts':
            return self.experts.named_parameters()
        else:
            raise Exception("Specified parameter type is not recognized: %s. Valid `param_type` includes: gate, local_experts." % param_type)

    def expert_local(self, x, reserve_shape):
        y = self.experts(x.view(x.size(0), x.size(1), *reserve_shape), self)
        self.protected_shape = y.shape
        return y.reshape(y.size(0), y.size(1), -1)

    def begin_record_routing(self):
        self.reset_record_routing()
        self.record_routing = True

    def end_record_routing(self):
        self.record_routing = False
    
    def reset_record_routing(self):
        # self.routing_records = torch.zeros(self.num_global_experts, dtype=torch.long, device=self.experts.batched_fc1_w.device)
        self.routing_records = torch.zeros(self.max_num_global_experts + 1, dtype=torch.long, device=self.experts.batched_fc1_w.device)
        self.sample_records = None

    def get_routing_records(self):
        return self.routing_records[:self.max_num_global_experts]
    
    def get_sample_records(self):
        return self.sample_records
    
    def dynamic_decoupled_gating(self, x, gate_logits):
        """
        åŠ¨æ€è§£è€¦é—¨æ§æœºåˆ¶ï¼šå°†ä¸“å®¶é€‰æ‹©ä¸è¾“å‡ºèåˆæƒé‡è§£è€¦
        """
        if not self.decoupled_gating:
            return gate_logits, None
            
        batch_size, seq_len, model_dim = x.shape
        x_flat = x.view(-1, model_dim)
        
        # 1. ä¸“å®¶é€‰æ‹©æƒé‡ (ç”¨äºè·¯ç”±å†³ç­–)
        selection_logits = self.selection_gate(x_flat)
        selection_logits = selection_logits / self.current_temperature  # æ¸©åº¦ç¼©æ”¾
        
        # 2. åŸºäºå†å²æ¿€æ´»æ¨¡å¼çš„åŠ¨æ€è°ƒæ•´
        expert_diversity_bonus = self._compute_diversity_bonus()
        selection_logits = selection_logits + expert_diversity_bonus.unsqueeze(0)
        
        # 3. è¾“å‡ºèåˆæƒé‡åŸºäºåŠ¨é‡æ›´æ–°
        current_selection = F.softmax(selection_logits, dim=-1)
        expert_activation = current_selection.mean(dim=0)  # å½“å‰æ‰¹æ¬¡çš„ä¸“å®¶æ¿€æ´»ç‡
        
        # åŠ¨é‡æ›´æ–°èåˆæƒé‡
        self.fusion_weights.data = (self.expert_momentum * self.fusion_weights + 
                                   (1 - self.expert_momentum) * expert_activation)
        
        # 4. æ›´æ–°ä¸“å®¶æ¿€æ´»å†å²
        self.expert_activation_history.data = (0.95 * self.expert_activation_history + 
                                              0.05 * expert_activation)
        self.expert_usage_count.data += (expert_activation > 0.01).float()
        
        return selection_logits, self.fusion_weights
    
    def _compute_diversity_bonus(self):
        """
        è®¡ç®—ä¸“å®¶å¤šæ ·æ€§å¥–åŠ±ï¼Œé¼“åŠ±ä½¿ç”¨ä¸æ´»è·ƒçš„ä¸“å®¶
        """
        # åŸºäºä½¿ç”¨é¢‘ç‡çš„é€†å‘å¥–åŠ±
        usage_normalized = self.expert_usage_count / (self.expert_usage_count.sum() + 1e-8)
        diversity_bonus = -torch.log(usage_normalized + 1e-8) * 0.01
        
        # ä¸ºä»æœªä½¿ç”¨çš„ä¸“å®¶æä¾›é¢å¤–å¥–åŠ±
        never_used_mask = (self.expert_usage_count == 0)
        diversity_bonus[never_used_mask] += 0.1
        
        return diversity_bonus
    
    def adaptive_loss_scheduling(self, base_aux_loss):
        """
        è‡ªé€‚åº”æŸå¤±è°ƒåº¦ï¼šè®­ç»ƒæ—©æœŸé¼“åŠ±æ¢ç´¢ï¼ŒåæœŸä¿è¯ç¨³å®š
        """
        if not self.adaptive_loss_schedule:
            return base_aux_loss
            
        step = self.training_step.item()
        
        # 1. è®¡ç®—å½“å‰è®­ç»ƒé˜¶æ®µçš„æŸå¤±æƒé‡
        if step < self.loss_schedule_warmup:
            # æ—©æœŸï¼šä½è¾…åŠ©æŸå¤± + æ¢ç´¢å¥–åŠ±
            progress = step / self.loss_schedule_warmup
            aux_weight = 0.001 + 0.009 * progress  # ä»0.001é€æ¸å¢åŠ åˆ°0.01
            
            # æ¢ç´¢å¥–åŠ±ï¼šé¼“åŠ±ä¸“å®¶åˆ†åŒ–
            exploration_bonus = self.exploration_bonus_init * (self.exploration_decay ** step)
            diversity_loss = self._compute_diversity_loss() * exploration_bonus
            
            total_loss = base_aux_loss * aux_weight + diversity_loss
            
        else:
            # åæœŸï¼šæ ‡å‡†è¾…åŠ©æŸå¤±æƒé‡ï¼Œæ³¨é‡ç¨³å®šæ€§
            aux_weight = 0.01 + 0.01 * min(1.0, (step - self.loss_schedule_warmup) / 10000)  # æœ€é«˜åˆ°0.02
            stability_loss = self._compute_stability_loss() * 0.005
            
            total_loss = base_aux_loss * aux_weight + stability_loss
        
        # æ›´æ–°å½“å‰è¾…åŠ©æŸå¤±æƒé‡ï¼ˆç”¨äºè®°å½•ï¼‰
        self.current_aux_weight.data = torch.tensor(aux_weight)
        
        return total_loss
    
    def _compute_diversity_loss(self):
        """
        è®¡ç®—å¤šæ ·æ€§æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶åˆ†åŒ–
        """
        # ä¸“å®¶æ¿€æ´»çš„æ–¹å·®è¶Šå¤§è¶Šå¥½ï¼ˆé¼“åŠ±åˆ†åŒ–ï¼‰
        activation_var = torch.var(self.expert_activation_history)
        diversity_loss = -torch.log(activation_var + 1e-8)  # è´Ÿå¯¹æ•°ï¼Œæ–¹å·®è¶Šå¤§æŸå¤±è¶Šå°
        
        # ä¸“å®¶ä½¿ç”¨å¹³è¡¡æ€§
        uniform_target = torch.ones_like(self.expert_activation_history) / len(self.expert_activation_history)
        balance_loss = F.kl_div(F.log_softmax(self.expert_activation_history, dim=0), 
                               uniform_target, reduction='batchmean')
        
        return diversity_loss + balance_loss
    
    def _compute_stability_loss(self):
        """
        è®¡ç®—ç¨³å®šæ€§æŸå¤±ï¼Œç¡®ä¿è®­ç»ƒåæœŸçš„ç¨³å®šæ€§
        """
        # ä¸“å®¶æ¿€æ´»ç‡çš„ç¨³å®šæ€§
        target_activation = 1.0 / self.num_global_experts
        stability_loss = F.mse_loss(self.expert_activation_history, 
                                   torch.full_like(self.expert_activation_history, target_activation))
        
        # é˜²æ­¢ä¸“å®¶æ¿€æ´»ç‡è¿‡äºé›†ä¸­
        max_activation = torch.max(self.expert_activation_history)
        concentration_penalty = F.relu(max_activation - 2 * target_activation) ** 2
        
        return stability_loss + concentration_penalty
    
    def update_temperature(self):
        """
        æ›´æ–°é—¨æ§æ¸©åº¦ï¼Œå®ç°è‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚
        """
        if self.enable_ddg_als:
            self.current_temperature.data *= self.temperature_decay
            self.current_temperature.data = torch.clamp(self.current_temperature.data, min=0.1, max=5.0)
    
    def get_ddg_als_stats(self):
        """
        è·å–DDG-ALSç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºç›‘æ§å’Œè°ƒè¯•
        """
        if not self.enable_ddg_als:
            return {}
            
        stats = {
            'training_step': self.training_step.item(),
            'current_temperature': self.current_temperature.item(),
            'current_aux_weight': self.current_aux_weight.item(),
            'expert_activation_std': torch.std(self.expert_activation_history).item(),
            'expert_max_activation': torch.max(self.expert_activation_history).item(),
            'expert_min_activation': torch.min(self.expert_activation_history).item(),
        }
        
        if self.decoupled_gating:
            stats.update({
                'fusion_weights_std': torch.std(self.fusion_weights).item(),
                'fusion_weights_max': torch.max(self.fusion_weights).item(),
                'fusion_weights_min': torch.min(self.fusion_weights).item(),
            })
            
        return stats
